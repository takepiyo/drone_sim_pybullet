{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.6 64-bit ('gym_drone_pybullet')",
   "metadata": {
    "interpreter": {
     "hash": "c11259c949eb2f5764dcd5f0cd07b8c1c16bf21e4c20f132e155b5f8b4b50f66"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import gym\n",
    "import gnwrapper\n",
    "from torch import nn\n",
    "\n",
    "from stable_baselines3 import A2C\n",
    "from stable_baselines3.a2c import MlpPolicy\n",
    "from stable_baselines3.common.env_checker import check_env\n",
    "\n",
    "from cpprb import ReplayBuffer\n",
    "\n",
    "from gym_pybullet_drones.utils.Logger import Logger\n",
    "from gym_pybullet_drones.envs.single_agent_rl.TakeoffAviary import TakeoffAviary\n",
    "from gym_pybullet_drones.utils.utils import sync, str2bool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "metadata": {},
     "execution_count": 2
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[INFO] BaseAviary.__init__() loaded parameters from the drone's .urdf:\n",
      "[INFO] m 0.027000, L 0.039700,\n",
      "[INFO] ixx 0.000014, iyy 0.000014, izz 0.000022,\n",
      "[INFO] kf 0.000000, km 0.000000,\n",
      "[INFO] t2w 2.250000, max_speed_kmh 30.000000,\n",
      "[INFO] gnd_eff_coeff 11.368590, prop_radius 0.023135,\n",
      "[INFO] drag_xy_coeff 0.000001, drag_z_coeff 0.000001,\n",
      "[INFO] dw_coeff_1 2267.180000, dw_coeff_2 0.160000, dw_coeff_3 -0.110000\n",
      "viewMatrix (-0.8660253882408142, -0.2499999701976776, 0.4330126941204071, 0.0, 0.0, 0.8660253286361694, 0.4999999701976776, 0.0, -0.4999999701976776, 0.4330126643180847, -0.75, 0.0, -0.0, 5.960464477539063e-08, -2.999999761581421, 1.0)\n",
      "projectionMatrix (1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, -1.0000200271606445, -1.0, 0.0, 0.0, -0.02000020071864128, 0.0)\n",
      "/home/takeshi/.pyenv/versions/gym_drone_pybullet/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n",
      "iter=  0 total reward: -850.9697\n"
     ]
    }
   ],
   "source": [
    "# ドローンをランダムに行動させ制御対象を把握する\n",
    "monitor_env = gnwrapper.Monitor(gym.make(\"takeoff-aviary-v0\", gui=True), size=(\n",
    "    400, 300), directory='.', force=True, video_callable=lambda ep: True)\n",
    "\n",
    "episode_max_steps = 500\n",
    "\n",
    "for episode_idx in range(1):\n",
    "    monitor_env.reset()\n",
    "    total_rew = 0.\n",
    "    for _ in range(episode_max_steps):\n",
    "        _, rew, done, _ = monitor_env.step(monitor_env.action_space.sample())\n",
    "        total_rew += rew\n",
    "        if done:\n",
    "            break\n",
    "    print(\"iter={0: 3d} total reward: {1: 4.4f}\".format(\n",
    "        episode_idx, total_rew))\n",
    "monitor_env.display()\n",
    "monitor_env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[INFO] BaseAviary.__init__() loaded parameters from the drone's .urdf:\n",
      "[INFO] m 0.027000, L 0.039700,\n",
      "[INFO] ixx 0.000014, iyy 0.000014, izz 0.000022,\n",
      "[INFO] kf 0.000000, km 0.000000,\n",
      "[INFO] t2w 2.250000, max_speed_kmh 30.000000,\n",
      "[INFO] gnd_eff_coeff 11.368590, prop_radius 0.023135,\n",
      "[INFO] drag_xy_coeff 0.000001, drag_z_coeff 0.000001,\n",
      "[INFO] dw_coeff_1 2267.180000, dw_coeff_2 0.160000, dw_coeff_3 -0.110000\n",
      "[INFO] Action space: Box(-1.0, 1.0, (4,), float32)\n",
      "[INFO] Observation space: Box(-1.0, 1.0, (12,), float32)\n",
      "<class 'gym_pybullet_drones.envs.single_agent_rl.TakeoffAviary.TakeoffAviary'>\n",
      "[ 0.7932858   0.9665094  -0.13613483 -0.11012445]\n",
      "<class 'numpy.ndarray'>\n",
      "/home/takeshi/.pyenv/versions/gym_drone_pybullet/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    }
   ],
   "source": [
    "# 行動空間と状態空間を確認する\n",
    "env = gym.make(\"takeoff-aviary-v0\")\n",
    "print(\"[INFO] Action space:\", env.action_space)\n",
    "print(\"[INFO] Observation space:\", env.observation_space)\n",
    "\n",
    "print(env.__class__)\n",
    "print(monitor_env.action_space.sample())\n",
    "print(type(monitor_env.action_space.sample()))"
   ]
  },
  {
   "source": [
    "ドローンの姿勢に関する報酬関数を定義する\n",
    "- rollとpitch角は0に近ほど良い\n",
    "- roll,pitch,yawの角速度は0に近いほど良い\n",
    "- ローターの推力の合計はドローンにかかる重力と釣り合っているほど良い(ローターの回転数を決める環境の場合(act: ActionType=ActionType.RPM)4つのactionを0.0にすることで重力と釣り合う推力になる)\n",
    "- ドローンの高度が0.1より小さい場合罰を与え、それ以上の場合は何も与えない(ホバリングするようにするため)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def angle_normalize(x):\n",
    "    return ((x + np.pi) % (2 * np.pi)) - np.pi\n",
    "\n",
    "def reward_fn(obses, acts):\n",
    "    pos, ang, vel, gyr = obses[:, 0:3], angle_normalize(obses[:, 3:6]), obses[:, 6:9], angle_normalize(obses[:, 9:12])\n",
    "    cost = np.zeros((pos.shape[0], ))\n",
    "    cost += np.sum(ang[:, 0:2] ** 2, axis=1)\n",
    "    cost += np.sum(gyr ** 2, axis=1) * 0.1\n",
    "    cost += np.sum(acts ** 2, axis=1) * 0.001\n",
    "    cost += (pos[:, 2] < 0.1).astype(np.float32)\n",
    "    return -cost"
   ]
  },
  {
   "source": [
    "ダイナミクスモデルの実装を2層のMLPで実装する\n",
    "\n",
    "現在の状態と次の状態の差分を予測する"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DynamicsModel(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, units=(32, 32)):\n",
    "        super().__init__()\n",
    "        self.model = torch.nn.Sequential(\n",
    "            torch.nn.Linear(input_dim, units[0]),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(units[0], units[1]),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(units[1], output_dim)\n",
    "        )\n",
    "\n",
    "        self.input_dim = input_dim\n",
    "\n",
    "        self._loss_fn = torch.nn.MSELoss(reduction='mean')\n",
    "        self._optimizer = torch.optim.Adam(self.model.parameters(), lr=0.001)\n",
    "\n",
    "    def predict(self, inputs):\n",
    "        assert inputs.shape[1] == self.input_dim\n",
    "        return self.model(inputs)\n",
    "\n",
    "    def fit(self, inputs, labels):\n",
    "        predicts = self.predict(inputs)\n",
    "        loss = self._loss_fn(predicts, labels)\n",
    "        self._optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self._optimizer.step()\n",
    "        return loss.data.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs_dim = 6  # 実機での適用に備え、姿勢制御に使える情報はオイラー角と角速度のみとした\n",
    "act_dim = env.action_space.high.size  # 各ローターのrpm(4)\n",
    "dynamics_model = DynamicsModel(input_dim=obs_dim+act_dim, output_dim=obs_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Sequential(\n  (0): Linear(in_features=10, out_features=32, bias=True)\n  (1): ReLU()\n  (2): Linear(in_features=32, out_features=32, bias=True)\n  (3): ReLU()\n  (4): Linear(in_features=32, out_features=6, bias=True)\n)\n"
     ]
    }
   ],
   "source": [
    "print(dynamics_model.model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_ang_and_gyr(obses): \n",
    "    return obses[:, 3:6], obses[:, 9:12]\n",
    "\n",
    "def format_output(diff_obses):\n",
    "    # print(diff_obses.shape)\n",
    "    formatted = np.zeros((diff_obses.shape[0], 12), dtype=np.float32)\n",
    "    formatted[:, 3:6] += diff_obses[:, 0:3]\n",
    "    formatted[:, 9:12] += diff_obses[:, 3:6]\n",
    "    return formatted\n",
    "\n",
    "def predict_next_state(obses, acts):\n",
    "    inputs = np.concatenate([*extract_ang_and_gyr(obses), acts], axis=1)\n",
    "    assert inputs.shape[1] == obs_dim + act_dim\n",
    "    inputs = torch.from_numpy(inputs).float()\n",
    "    diff_obses = dynamics_model.predict(inputs).data.numpy()\n",
    "    next_obses = obses + format_output(diff_obses)\n",
    "    return next_obses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10Kデータ分 (s, a, s') を保存できるリングバッファを用意します\n",
    "rb_dict = {\n",
    "    \"size\": 10000,\n",
    "    \"default_dtype\": np.float32,\n",
    "    \"env_dict\": {\n",
    "        \"obs\": {\"shape\": env.observation_space.shape},\n",
    "        \"next_obs\": {\"shape\": env.observation_space.shape},\n",
    "        \"act\": {\"shape\": env.action_space.shape}}}\n",
    "dynamics_buffer = ReplayBuffer(**rb_dict)"
   ]
  },
  {
   "source": [
    "RSの実装"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomPolicy:\n",
    "    def __init__(self, max_action, act_dim):\n",
    "        self._max_action = max_action  # action の最大値\n",
    "        self._act_dim = act_dim  # action の次元数\n",
    "\n",
    "    def get_actions(self, batch_size):\n",
    "        # 一様分布からバッチサイズ分ランダムにサンプリング\n",
    "        return np.random.uniform(\n",
    "            low=-self._max_action,\n",
    "            high=self._max_action,\n",
    "            size=(batch_size, self._act_dim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy = RandomPolicy(\n",
    "    max_action=env.action_space.high[0],\n",
    "    act_dim=env.action_space.high.size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_shooting(init_obs, n_mpc_episodes=64, horizon=20):\n",
    "    init_actions = policy.get_actions(batch_size=n_mpc_episodes)\n",
    "    returns = np.zeros(shape=(n_mpc_episodes, ))\n",
    "    obses = np.tile(init_obs, (n_mpc_episodes, 1))\n",
    "\n",
    "    for i in range(horizon):\n",
    "        if i == 0:\n",
    "            acts = np.copy(init_actions)\n",
    "        else:\n",
    "            acts = policy.get_actions(batch_size=n_mpc_episodes)\n",
    "        \n",
    "        next_obses = predict_next_state(obses, acts)\n",
    "\n",
    "        rewards = reward_fn(obses, acts)\n",
    "\n",
    "        returns += rewards\n",
    "        obses = next_obses\n",
    "\n",
    "    return init_actions[np.argmax(returns)] \n",
    "        "
   ]
  },
  {
   "source": [
    "RSの実行"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "iter=  0 total steps:  2500 total reward: -550.7441 mean loss: 0.750457\n",
      "iter=  5 total steps:  5000 total reward: -550.6444 mean loss: 1.333298\n",
      "iter= 10 total steps:  7500 total reward: -550.8623 mean loss: 1.750115\n",
      "iter= 15 total steps:  10000 total reward: -550.7286 mean loss: 2.090773\n",
      "iter= 20 total steps:  12500 total reward: -551.1164 mean loss: 2.266227\n",
      "iter= 25 total steps:  15000 total reward: -550.8788 mean loss: 2.226449\n",
      "iter= 30 total steps:  17500 total reward: -552.2176 mean loss: 2.159709\n",
      "iter= 35 total steps:  20000 total reward: -550.6050 mean loss: 2.160700\n",
      "iter= 40 total steps:  22500 total reward: -550.9187 mean loss: 2.268069\n",
      "iter= 45 total steps:  25000 total reward: -550.8514 mean loss: 2.244724\n",
      "iter= 50 total steps:  27500 total reward: -550.8348 mean loss: 2.384778\n",
      "iter= 55 total steps:  30000 total reward: -550.7140 mean loss: 2.357429\n",
      "iter= 60 total steps:  32500 total reward: -550.7693 mean loss: 2.155552\n",
      "iter= 65 total steps:  35000 total reward: -550.7297 mean loss: 2.195693\n",
      "iter= 70 total steps:  37500 total reward: -550.6855 mean loss: 2.217902\n",
      "iter= 75 total steps:  40000 total reward: -550.7292 mean loss: 2.164663\n",
      "iter= 80 total steps:  42500 total reward: -550.7236 mean loss: 2.266464\n",
      "iter= 85 total steps:  45000 total reward: -550.6471 mean loss: 2.399291\n",
      "iter= 90 total steps:  47500 total reward: -550.5944 mean loss: 2.425160\n",
      "iter= 95 total steps:  50000 total reward: -551.3855 mean loss: 2.356867\n"
     ]
    }
   ],
   "source": [
    "batch_size = 100\n",
    "n_episodes = 100\n",
    "\n",
    "def fit_dynamics(n_iter=50):\n",
    "    mean_loss = 0.\n",
    "    for _ in range(n_iter):\n",
    "        samples = dynamics_buffer.sample(batch_size)\n",
    "        inputs = np.concatenate([*extract_ang_and_gyr(samples['obs']), samples['act']], axis=1)\n",
    "        labels = np.concatenate([*extract_ang_and_gyr(samples['next_obs'])], axis=1) - np.concatenate([*extract_ang_and_gyr(samples['obs'])], axis=1)\n",
    "        mean_loss += dynamics_model.fit(\n",
    "            torch.from_numpy(inputs).float(),\n",
    "            torch.from_numpy(labels).float()\n",
    "        )\n",
    "    return mean_loss\n",
    "\n",
    "total_steps = 0\n",
    "\n",
    "for _ in range(10):\n",
    "    obs = env.reset()\n",
    "    for _ in range(200):\n",
    "        total_steps += 1\n",
    "        act = env.action_space.sample()\n",
    "        next_obs, _, done, _ = env.step(act)\n",
    "        dynamics_buffer.add(obs=obs, act=act, next_obs=next_obs)\n",
    "        obs = next_obs\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "fit_dynamics(n_iter=1000)\n",
    "\n",
    "for episode_idx in range(n_episodes):\n",
    "    total_rew = 0.\n",
    "\n",
    "    obs = env.reset()\n",
    "    for _ in range(episode_max_steps):\n",
    "        total_steps += 1\n",
    "\n",
    "        act = random_shooting(obs)\n",
    "        next_obs, _, done, _ = env.step(act)\n",
    "\n",
    "        rew = reward_fn(obs.reshape(1, obs.shape[0]), act.reshape(1, act.shape[0])) \n",
    "\n",
    "        dynamics_buffer.add(obs=obs, act=act, next_obs=next_obs)\n",
    "\n",
    "        total_rew += float(rew[0])\n",
    "        if done:\n",
    "            break\n",
    "        obs = next_obs\n",
    "\n",
    "    mean_loss = fit_dynamics(n_iter=100)\n",
    "    if episode_idx % 5 == 0:\n",
    "        print(\"iter={0: 3d} total steps: {1: 5d} total reward: {2: 4.4f} mean loss: {3:.6f}\".format(\n",
    "            episode_idx, total_steps, total_rew, mean_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "iter=  0 total reward: -458.2892\n",
      "iter=  1 total reward: -838.5072\n",
      "iter=  2 total reward: -429.2932\n"
     ]
    }
   ],
   "source": [
    "for episode_idx in range(3):\n",
    "    obs = monitor_env.reset()\n",
    "    total_rew = 0.\n",
    "    for _ in range(episode_max_steps):\n",
    "        act = random_shooting(obs)\n",
    "        next_obs, rew, done, _ = monitor_env.step(act)\n",
    "        total_rew += rew\n",
    "        if done:\n",
    "            break\n",
    "        obs = next_obs\n",
    "    print(\"iter={0: 3d} total reward: {1: 4.4f}\".format(episode_idx, total_rew))\n",
    "\n",
    "monitor_env.display(reset=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}